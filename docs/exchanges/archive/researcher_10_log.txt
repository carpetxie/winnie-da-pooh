Here's a summary of what was accomplished in iteration 10:

## Iteration 10 Summary

### Key Code Changes
1. **Fixed interior-only partition inconsistency** in Phase 7E (`experiment13/run.py`) — the old code used different median splits for tail-aware vs interior-only, creating different event groups. Now both use the same partition.
   - **Important correction**: CPI high-surprise interior went from 0.86 (below 1.0!) to **1.13** — the previous "below 1.0" finding was an artifact.

2. **Added CRPS/uniform ratio for surprise subsets** — a mechanically independent metric that doesn't share the MAE denominator circularity. Results:
   - CPI: high-surprise **1.42** vs low-surprise **2.34** → confirms pattern is genuine
   - JC: high-surprise **0.69** vs low-surprise **1.07** → confirms pattern is genuine

3. **Added two-panel scatter plot** (`surprise_vs_crps_mae.png`) showing surprise magnitude vs CRPS/MAE with Spearman ρ annotations.

### Paper Changes
- **Abstract**: Added CRPS/uniform confirmation; added Appendix C reference ("13 invalidated findings")
- **Practical takeaways**: Fixed imprecise "34% CRPS improvement" language; updated CPI takeaway with CRPS/uniform numbers
- **Executive summary table**: Added CRPS/uniform row
- **Surprise Magnitude section**: Complete rewrite — corrected interior numbers, added mechanical vs informative decomposition, added absolute CRPS comparison, downgraded "most actionable finding" language
- **Temporal CIs**: Specified "percentile method" bootstrap

### Most Important Finding
The CRPS/uniform comparison definitively answers the reviewer's main concern: the surprise-magnitude pattern is **genuine, not purely mechanical**. High-surprise distributions are better calibrated by a metric independent of the MAE denominator. But CPI high-surprise CRPS/uniform still exceeds 1.0, so the improvement is relative (less harmful), not absolute (actually good) — reported honestly.
