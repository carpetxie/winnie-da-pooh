The critique has been written to `docs/exchanges/critique_latest.md`. Here's a summary of the key findings:

## Key Finding: BCa Bootstrap Bug

The paper's most consequential issue is a **silent code bug** in `experiment13/run.py`. The `_ratio_of_means` function has a signature incompatible with `scipy.stats.bootstrap`'s BCa method, causing a `TypeError` that's silently caught by a bare `except Exception` block. The code falls back to percentile bootstrap, but the paper claims BCa throughout.

**This matters because:**
- With percentile CIs: JC CRPS/MAE = 0.60 [0.45, 0.78] — **excludes 1.0** ✓
- With BCa CIs (corrected): JC CRPS/MAE = 0.60 [0.37, 1.02] — **includes 1.0** ✗

The paper's strongest statistical claim ("CI excludes 1.0, confirming distributional value") depends on the wrong bootstrap method.

## Recommended Fixes
1. **Fix the function signature** (the corrected code is provided in the critique)
2. **Add a binomial sign test** (12/16 JC events have ratio < 1.0 → p = 0.038) as a method-independent significance test
3. **Report both BCa and percentile CIs** for transparency
4. **Update the paper's language** to "strongly directional" rather than "statistically significant"

The core argument survives — JC distributions do appear to add value — but the statistical confidence level needs honest recalibration.
